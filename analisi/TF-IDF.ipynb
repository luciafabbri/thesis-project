{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91dce393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import numpy as np\n",
    "import copy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d1f619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commento</th>\n",
       "      <th>valutazione</th>\n",
       "      <th>tossicita</th>\n",
       "      <th>age_group</th>\n",
       "      <th>gender</th>\n",
       "      <th>utente_id</th>\n",
       "      <th>comment_cleaned</th>\n",
       "      <th>emoji_set</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>parole_maiusc</th>\n",
       "      <th>...</th>\n",
       "      <th>SMOGIndex</th>\n",
       "      <th>RIX</th>\n",
       "      <th>DaleChallIndex</th>\n",
       "      <th>comment_tokenized</th>\n",
       "      <th>comment_tok=2</th>\n",
       "      <th>comment_tok=3</th>\n",
       "      <th>comment_lemmatized</th>\n",
       "      <th>VAD_valence</th>\n",
       "      <th>VAD_arousal</th>\n",
       "      <th>VAD_dominance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is this a satire subreddit?</td>\n",
       "      <td>Non toxic</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>satire subreddit</td>\n",
       "      <td>set()</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>8.477226</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.889200</td>\n",
       "      <td>['satire', 'subreddit']</td>\n",
       "      <td>[('satire', 'subreddit')]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['satire', 'subreddit']</td>\n",
       "      <td>-0.084000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>-0.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dayum.</td>\n",
       "      <td>Non toxic</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>dayum</td>\n",
       "      <td>set()</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.839600</td>\n",
       "      <td>['dayum']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['dayum']</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And they wouldnâ€™t be scrubbing the floors if t...</td>\n",
       "      <td>Non toxic</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>would scrubbing floors employed</td>\n",
       "      <td>set()</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.040900</td>\n",
       "      <td>['would', 'scrubbing', 'floors', 'employed']</td>\n",
       "      <td>[('would', 'scrubbing'), ('scrubbing', 'floors...</td>\n",
       "      <td>[('would', 'scrubbing', 'floors'), ('scrubbing...</td>\n",
       "      <td>['would', 'scrub', 'floor', 'employ']</td>\n",
       "      <td>-0.102000</td>\n",
       "      <td>0.282500</td>\n",
       "      <td>-0.358500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good point. I mean does Nebraska really exist?...</td>\n",
       "      <td>Non toxic</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>good point mean nebraska really exist ever met...</td>\n",
       "      <td>set()</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>10.745967</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.233000</td>\n",
       "      <td>['good', 'point', 'mean', 'nebraska', 'really'...</td>\n",
       "      <td>[('good', 'point'), ('point', 'mean'), ('mean'...</td>\n",
       "      <td>[('good', 'point', 'mean'), ('point', 'mean', ...</td>\n",
       "      <td>['good', 'point', 'mean', 'nebraska', 'really'...</td>\n",
       "      <td>0.326000</td>\n",
       "      <td>-0.267333</td>\n",
       "      <td>0.086333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>or a yuumi/nami/lulu/zilean/ any enhancer play...</td>\n",
       "      <td>Non toxic</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>yuumi nami lulu zilean enhancer plays team</td>\n",
       "      <td>set()</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>8.477226</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.881486</td>\n",
       "      <td>['yuumi', 'nami', 'lulu', 'zilean', 'enhancer'...</td>\n",
       "      <td>[('yuumi', 'nami'), ('nami', 'lulu'), ('lulu',...</td>\n",
       "      <td>[('yuumi', 'nami', 'lulu'), ('nami', 'lulu', '...</td>\n",
       "      <td>['yuumi', 'nami', 'lulu', 'zilean', 'enhancer'...</td>\n",
       "      <td>0.687000</td>\n",
       "      <td>0.294500</td>\n",
       "      <td>0.521500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5270</th>\n",
       "      <td>Commies are going to commie. These people hate...</td>\n",
       "      <td>Extremely violent</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>211</td>\n",
       "      <td>commies going commie people hate want die</td>\n",
       "      <td>set()</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.858629</td>\n",
       "      <td>['commies', 'going', 'commie', 'people', 'hate...</td>\n",
       "      <td>[('commies', 'going'), ('going', 'commie'), ('...</td>\n",
       "      <td>[('commies', 'going', 'commie'), ('going', 'co...</td>\n",
       "      <td>['commie', 'go', 'commie', 'people', 'hate', '...</td>\n",
       "      <td>-0.329714</td>\n",
       "      <td>0.375857</td>\n",
       "      <td>0.045000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5271</th>\n",
       "      <td>Yeah but then the good guys win and rape you t...</td>\n",
       "      <td>Extremely violent</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>211</td>\n",
       "      <td>yeah good guys win rape death occupy nation in...</td>\n",
       "      <td>set()</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>10.745967</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.973067</td>\n",
       "      <td>['yeah', 'good', 'guys', 'win', 'rape', 'death...</td>\n",
       "      <td>[('yeah', 'good'), ('good', 'guys'), ('guys', ...</td>\n",
       "      <td>[('yeah', 'good', 'guys'), ('good', 'guys', 'w...</td>\n",
       "      <td>['yeah', 'good', 'guy', 'win', 'rape', 'death'...</td>\n",
       "      <td>0.012750</td>\n",
       "      <td>0.015250</td>\n",
       "      <td>0.121125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5272</th>\n",
       "      <td>If you're white and use your gun to defend you...</td>\n",
       "      <td>A little violent</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>211</td>\n",
       "      <td>white use gun defend arrested racist get life ...</td>\n",
       "      <td>set()</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>8.477226</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.955289</td>\n",
       "      <td>['white', 'use', 'gun', 'defend', 'arrested', ...</td>\n",
       "      <td>[('white', 'use'), ('use', 'gun'), ('gun', 'de...</td>\n",
       "      <td>[('white', 'use', 'gun'), ('use', 'gun', 'defe...</td>\n",
       "      <td>['white', 'use', 'gun', 'defend', 'arrest', 'r...</td>\n",
       "      <td>-0.115429</td>\n",
       "      <td>0.365714</td>\n",
       "      <td>0.212857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5273</th>\n",
       "      <td>I will rape Elon Musk</td>\n",
       "      <td>A little violent</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>211</td>\n",
       "      <td>rape elon musk</td>\n",
       "      <td>set()</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.938800</td>\n",
       "      <td>['rape', 'elon', 'musk']</td>\n",
       "      <td>[('rape', 'elon'), ('elon', 'musk')]</td>\n",
       "      <td>[('rape', 'elon', 'musk')]</td>\n",
       "      <td>['rape', 'elon', 'musk']</td>\n",
       "      <td>-0.469000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>-0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5274</th>\n",
       "      <td>HE SHOULD BE ARRESTED!!!!\\n\\nLetâ€™s riot and bu...</td>\n",
       "      <td>Extremely violent</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>211</td>\n",
       "      <td>arrested let us riot burn city</td>\n",
       "      <td>set()</td>\n",
       "      <td>0</td>\n",
       "      <td>['HE', 'SHOULD', 'BE', 'ARRESTED!!!!']</td>\n",
       "      <td>...</td>\n",
       "      <td>8.477226</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.560933</td>\n",
       "      <td>['arrested', 'let', 'us', 'riot', 'burn', 'city']</td>\n",
       "      <td>[('arrested', 'let'), ('let', 'us'), ('us', 'r...</td>\n",
       "      <td>[('arrested', 'let', 'us'), ('let', 'us', 'rio...</td>\n",
       "      <td>['arrest', 'let', 'u', 'riot', 'burn', 'city']</td>\n",
       "      <td>-0.111800</td>\n",
       "      <td>0.253200</td>\n",
       "      <td>0.108400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5275 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               commento        valutazione  \\\n",
       "0                           is this a satire subreddit?          Non toxic   \n",
       "1                                                Dayum.          Non toxic   \n",
       "2     And they wouldnâ€™t be scrubbing the floors if t...          Non toxic   \n",
       "3     Good point. I mean does Nebraska really exist?...          Non toxic   \n",
       "4     or a yuumi/nami/lulu/zilean/ any enhancer play...          Non toxic   \n",
       "...                                                 ...                ...   \n",
       "5270  Commies are going to commie. These people hate...  Extremely violent   \n",
       "5271  Yeah but then the good guys win and rape you t...  Extremely violent   \n",
       "5272  If you're white and use your gun to defend you...   A little violent   \n",
       "5273                              I will rape Elon Musk   A little violent   \n",
       "5274  HE SHOULD BE ARRESTED!!!!\\n\\nLetâ€™s riot and bu...  Extremely violent   \n",
       "\n",
       "      tossicita  age_group  gender  utente_id  \\\n",
       "0             0        1.0       1          1   \n",
       "1             0        1.0       1          1   \n",
       "2             0        1.0       1          1   \n",
       "3             0        1.0       1          1   \n",
       "4             0        1.0       1          1   \n",
       "...         ...        ...     ...        ...   \n",
       "5270          3        1.0       0        211   \n",
       "5271          3        1.0       0        211   \n",
       "5272          1        1.0       0        211   \n",
       "5273          1        1.0       0        211   \n",
       "5274          3        1.0       0        211   \n",
       "\n",
       "                                        comment_cleaned emoji_set  \\\n",
       "0                                      satire subreddit     set()   \n",
       "1                                                 dayum     set()   \n",
       "2                       would scrubbing floors employed     set()   \n",
       "3     good point mean nebraska really exist ever met...     set()   \n",
       "4            yuumi nami lulu zilean enhancer plays team     set()   \n",
       "...                                                 ...       ...   \n",
       "5270          commies going commie people hate want die     set()   \n",
       "5271  yeah good guys win rape death occupy nation in...     set()   \n",
       "5272  white use gun defend arrested racist get life ...     set()   \n",
       "5273                                     rape elon musk     set()   \n",
       "5274                     arrested let us riot burn city     set()   \n",
       "\n",
       "      emoji_count                           parole_maiusc  ...  SMOGIndex  \\\n",
       "0               0                                      []  ...   8.477226   \n",
       "1               0                                      []  ...   3.000000   \n",
       "2               0                                      []  ...   3.000000   \n",
       "3               0                                      []  ...  10.745967   \n",
       "4               0                                      []  ...   8.477226   \n",
       "...           ...                                     ...  ...        ...   \n",
       "5270            0                                      []  ...   3.000000   \n",
       "5271            0                                      []  ...  10.745967   \n",
       "5272            0                                      []  ...   8.477226   \n",
       "5273            0                                      []  ...   3.000000   \n",
       "5274            0  ['HE', 'SHOULD', 'BE', 'ARRESTED!!!!']  ...   8.477226   \n",
       "\n",
       "      RIX DaleChallIndex                                  comment_tokenized  \\\n",
       "0     1.0      15.889200                            ['satire', 'subreddit']   \n",
       "1     0.0      15.839600                                          ['dayum']   \n",
       "2     2.0      12.040900       ['would', 'scrubbing', 'floors', 'employed']   \n",
       "3     3.0       5.233000  ['good', 'point', 'mean', 'nebraska', 'really'...   \n",
       "4     1.0      13.881486  ['yuumi', 'nami', 'lulu', 'zilean', 'enhancer'...   \n",
       "...   ...            ...                                                ...   \n",
       "5270  1.0       4.858629  ['commies', 'going', 'commie', 'people', 'hate...   \n",
       "5271  1.0      10.973067  ['yeah', 'good', 'guys', 'win', 'rape', 'death...   \n",
       "5272  1.0       3.955289  ['white', 'use', 'gun', 'defend', 'arrested', ...   \n",
       "5273  0.0      15.938800                           ['rape', 'elon', 'musk']   \n",
       "5274  1.0       5.560933  ['arrested', 'let', 'us', 'riot', 'burn', 'city']   \n",
       "\n",
       "                                          comment_tok=2  \\\n",
       "0                             [('satire', 'subreddit')]   \n",
       "1                                                    []   \n",
       "2     [('would', 'scrubbing'), ('scrubbing', 'floors...   \n",
       "3     [('good', 'point'), ('point', 'mean'), ('mean'...   \n",
       "4     [('yuumi', 'nami'), ('nami', 'lulu'), ('lulu',...   \n",
       "...                                                 ...   \n",
       "5270  [('commies', 'going'), ('going', 'commie'), ('...   \n",
       "5271  [('yeah', 'good'), ('good', 'guys'), ('guys', ...   \n",
       "5272  [('white', 'use'), ('use', 'gun'), ('gun', 'de...   \n",
       "5273               [('rape', 'elon'), ('elon', 'musk')]   \n",
       "5274  [('arrested', 'let'), ('let', 'us'), ('us', 'r...   \n",
       "\n",
       "                                          comment_tok=3  \\\n",
       "0                                                    []   \n",
       "1                                                    []   \n",
       "2     [('would', 'scrubbing', 'floors'), ('scrubbing...   \n",
       "3     [('good', 'point', 'mean'), ('point', 'mean', ...   \n",
       "4     [('yuumi', 'nami', 'lulu'), ('nami', 'lulu', '...   \n",
       "...                                                 ...   \n",
       "5270  [('commies', 'going', 'commie'), ('going', 'co...   \n",
       "5271  [('yeah', 'good', 'guys'), ('good', 'guys', 'w...   \n",
       "5272  [('white', 'use', 'gun'), ('use', 'gun', 'defe...   \n",
       "5273                         [('rape', 'elon', 'musk')]   \n",
       "5274  [('arrested', 'let', 'us'), ('let', 'us', 'rio...   \n",
       "\n",
       "                                     comment_lemmatized  VAD_valence  \\\n",
       "0                               ['satire', 'subreddit']    -0.084000   \n",
       "1                                             ['dayum']     0.000000   \n",
       "2                 ['would', 'scrub', 'floor', 'employ']    -0.102000   \n",
       "3     ['good', 'point', 'mean', 'nebraska', 'really'...     0.326000   \n",
       "4     ['yuumi', 'nami', 'lulu', 'zilean', 'enhancer'...     0.687000   \n",
       "...                                                 ...          ...   \n",
       "5270  ['commie', 'go', 'commie', 'people', 'hate', '...    -0.329714   \n",
       "5271  ['yeah', 'good', 'guy', 'win', 'rape', 'death'...     0.012750   \n",
       "5272  ['white', 'use', 'gun', 'defend', 'arrest', 'r...    -0.115429   \n",
       "5273                           ['rape', 'elon', 'musk']    -0.469000   \n",
       "5274     ['arrest', 'let', 'u', 'riot', 'burn', 'city']    -0.111800   \n",
       "\n",
       "      VAD_arousal  VAD_dominance  \n",
       "0        0.310000      -0.056000  \n",
       "1        0.000000       0.000000  \n",
       "2        0.282500      -0.358500  \n",
       "3       -0.267333       0.086333  \n",
       "4        0.294500       0.521500  \n",
       "...           ...            ...  \n",
       "5270     0.375857       0.045000  \n",
       "5271     0.015250       0.121125  \n",
       "5272     0.365714       0.212857  \n",
       "5273     0.310000      -0.020000  \n",
       "5274     0.253200       0.108400  \n",
       "\n",
       "[5275 rows x 32 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    \"./survey_preprocessed.csv\",\n",
    "    sep=\";\",\n",
    "    quoting=csv.QUOTE_MINIMAL,\n",
    "    encoding=\"utf-8\",\n",
    "    engine=\"python\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c21f241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['commento', 'valutazione', 'tossicita', 'age_group', 'gender',\n",
       "       'utente_id', 'comment_cleaned', 'emoji_set', 'emoji_count',\n",
       "       'parole_maiusc', 'pos_tag', 'nrc_affect_dict', 'nrc_raw_scores',\n",
       "       'nrc_top_emotions', 'nrc_affect_frequencies', 'comm_polarita',\n",
       "       'Kincaid', 'ARI', 'Coleman-Liau', 'FleschReadingEase',\n",
       "       'GunningFogIndex', 'LIX', 'SMOGIndex', 'RIX', 'DaleChallIndex',\n",
       "       'comment_tokenized', 'comment_tok=2', 'comment_tok=3',\n",
       "       'comment_lemmatized', 'VAD_valence', 'VAD_arousal', 'VAD_dominance'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5b9132",
   "metadata": {},
   "source": [
    "### CONTEGGIO PAROLE PIÃ™ FREQUENTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2615d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "all_tokens = [token for list in df['comment_tokenized'] for token in list]\n",
    "freq_dist = FreqDist(all_tokens)\n",
    "\n",
    "freq_dist.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7f677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist.plot(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d4c9f",
   "metadata": {},
   "source": [
    "### DISPERSIONE DELLE PAROLE NEL TESTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3ed2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text = nltk.Text(all_tokens)\n",
    "\n",
    "# grafico che mostra la dispersione di una parola all'interno del testo (inizio, fne, centrale)\n",
    "nltk_text.dispersion_plot(['people','fuck','go', 'shit','like', 'fucking', 'would'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cbb14e",
   "metadata": {},
   "source": [
    "### TF-IDF (parole piÃ¹ importanti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36591424",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"comment_uniti\"] = df[\"comment_tokenized\"].apply(lambda tokens: \" \".join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3f120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer1 = TfidfVectorizer()\n",
    "X = vectorizer1.fit_transform(df[\"comment_uniti\"])\n",
    "feature_names1 = vectorizer1.get_feature_names_out()\n",
    "tfidf_df1 = pd.DataFrame(X.toarray(), columns=vectorizer1.get_feature_names_out())\n",
    "tfidf_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8112c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_scores = np.asarray(X.sum(axis=0)).ravel()\n",
    "sorted_indices = word_scores.argsort()[::-1]\n",
    "\n",
    "N = 20\n",
    "top_words = [(feature_names1[i], word_scores[i]) for i in sorted_indices[:N]]\n",
    "print(\"Top parole globali:\", top_words)\n",
    "\n",
    "words, scores = zip(*top_words)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(words, scores)\n",
    "plt.xlabel(\"TF-IDF Score (sommato su tutti i documenti)\")\n",
    "plt.title(f\"Top {N} parole nel corpus\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5b3929",
   "metadata": {},
   "source": [
    "considero anche bigrammi o trigrammi, consideri anche sequenze di 2 o 3 parole consecutive come feature, per vedere se ci sono delle parole che singolarmente non hanno significato, ma insieme lo acquisiscono (es. New York)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fa807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = TfidfVectorizer(ngram_range=(2,3)) \n",
    "X = vectorizer2.fit_transform(df[\"comment_uniti\"])\n",
    "feature_names2 = vectorizer2.get_feature_names_out()\n",
    "tfidf_df2 = pd.DataFrame(X.toarray(), columns=vectorizer2.get_feature_names_out())\n",
    "tfidf_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a3a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_scores = np.asarray(X.sum(axis=0)).ravel()\n",
    "sorted_indices = word_scores.argsort()[::-1]\n",
    "\n",
    "N = 20\n",
    "top_words = [(feature_names2[i], word_scores[i]) for i in sorted_indices[:N]]\n",
    "print(\"Top parole globali:\", top_words)\n",
    "\n",
    "words, scores = zip(*top_words)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(words, scores)\n",
    "plt.xlabel(\"TF-IDF Score (sommato su tutti i documenti)\")\n",
    "plt.title(f\"Top {N} parole nel corpus\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c228dbaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8768a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['comment_uniti'], inplace=True)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
